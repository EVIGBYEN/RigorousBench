{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50172f6b",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d58314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, re\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d9ca7",
   "metadata": {},
   "source": [
    "# Definition of API Call Class and I/O Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of Deep Research Agents Model (using o4-mini-deep-research as an example)\n",
    "class DRM:\n",
    "    def __init__(self, model):\n",
    "        self.client = openai.OpenAI(api_key=\"OPENAI_API_KEY\", timeout=3600)\n",
    "        self.model = model\n",
    "    def __call__(self, uprompt):\n",
    "        response = self.client.responses.create(model=self.model, input=uprompt, tools=[{\"type\": \"web_search_preview\"}])\n",
    "        return response\n",
    "\n",
    "# Definition of Judger Model\n",
    "class GPT:\n",
    "    def __init__(self, model, sprompt=\"\"):\n",
    "        self.client = openai.OpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "        self.model = model\n",
    "        self.sprompt = sprompt\n",
    "    def __call__(self, uprompt):\n",
    "        messages = [{\"role\": \"system\", \"content\": self.sprompt}]\n",
    "        messages.append({\"role\": \"user\", \"content\": uprompt})\n",
    "        completion = self.client.chat.completions.create(model=self.model, messages=messages, temperature=0)\n",
    "        response = completion.choices[0].message.content\n",
    "        return response\n",
    "\n",
    "drm = DRM(\"o4-mini-deep-research-2025-06-26\")\n",
    "gpt = GPT(\"gpt-4o-2024-11-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56cf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of I/O Paths\n",
    "bench_file_path = \"RigorousBench.jsonl\"\n",
    "log_file_path = \"Log_o4-mini-deep-research-2025-06-26.jsonl\"\n",
    "eval_file_path = \"Eval_o4-mini-deep-research-2025-06-26.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723ce8c",
   "metadata": {},
   "source": [
    "# Get Response Report and Save Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31903724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Mechanism After Interruption\n",
    "completed_list = []\n",
    "with open(log_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        log_data = json.loads(line.strip())\n",
    "        completed_list.append(log_data[\"uid\"])\n",
    "\n",
    "# Load Benchmark\n",
    "bench = []\n",
    "with open(bench_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line.strip())\n",
    "        bench.append(entry)\n",
    "\n",
    "# Get Response Report and Metadata\n",
    "for i in range(len(bench)):\n",
    "    # Retry Mechanism\n",
    "    for j in range(3):\n",
    "        try:\n",
    "            entry = bench[i]\n",
    "            uid = entry[\"uid\"]\n",
    "            query = entry[\"query\"]\n",
    "            qsr = entry[\"qsr\"]\n",
    "            tsl = entry[\"tsl\"]\n",
    "            fak = entry[\"fak\"]\n",
    "            fdk = entry[\"fdk\"]\n",
    "            if uid in completed_list:\n",
    "                print(f\"NOTE: Log for UID {uid} already exists\")\n",
    "            else:\n",
    "                print(f\"START ### UID {uid}\")\n",
    "                # Get Deep Research Response\n",
    "                response = drm(query)\n",
    "                # Log of Usage\n",
    "                input_tokens = response.usage.input_tokens\n",
    "                output_tokens = response.usage.output_tokens\n",
    "                reasoning_tokens = response.usage.output_tokens_details.reasoning_tokens\n",
    "                total_tokens = response.usage.total_tokens\n",
    "                # Log of Report and Annotations\n",
    "                for item in response.output:\n",
    "                    if type(item) == openai.types.responses.response_output_message.ResponseOutputMessage:\n",
    "                        annotations = item.content[0].annotations\n",
    "                        annotations = [annotation.url for annotation in annotations if hasattr(annotation, 'url') and annotation.url]\n",
    "                        pure_annotations = [re.sub(r'#:~:text=.*$', '', url) for url in annotations]\n",
    "                        text = item.content[0].text\n",
    "                        pure_text = re.sub(r'\\(\\[[^\\]]+\\]\\([^)]+\\)\\)', '', text)\n",
    "                # Log Entry\n",
    "                log = {\"uid\": uid, \"query\": query, \"qsr\": qsr, \"tsl\": tsl, \"fak\": fak, \"fdk\": fdk,\n",
    "                    \"input_tokens\": input_tokens, \"output_tokens\": output_tokens, \n",
    "                    \"reasoning_tokens\": reasoning_tokens, \"total_tokens\": total_tokens,\n",
    "                    \"annotations\": annotations, \"pure_annotations\": pure_annotations,\n",
    "                    \"text\": text, \"pure_text\": pure_text}\n",
    "                # Save Log Entry\n",
    "                with open(log_file_path, 'a', encoding='utf-8') as outfile:\n",
    "                    outfile.write(json.dumps(log) + '\\n')\n",
    "                print(f\"Log for UID {uid} has been saved.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Failure #{j+1}: {e}\")\n",
    "            time.sleep(10)\n",
    "    else:\n",
    "        print(\"Exceeded preset retry limit. Abandoning attempt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed99d4",
   "metadata": {},
   "source": [
    "# Multidimensional Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980262d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRRs List\n",
    "grr = [\n",
    "    \"Does the report include a clear three-part structure (introduction, body, conclusion)? Yes=2, No=0\",\n",
    "    \"Does the report clearly state the research question or objective at the beginning? Yes=2, No=0\",\n",
    "    \"Does the report provide background and purpose in the introduction? Yes=1, No=0\",\n",
    "    \"Does the report develop coherent arguments in the body section? Yes=2, No=0\",\n",
    "    \"Does the report summarize key findings in the conclusion? Yes=2, No=0\",\n",
    "    \"Does the report offer actionable recommendations or future directions? Yes=2, No=0\",\n",
    "    \"Does the report use smooth transitions between paragraphs or sections? Yes=1, No=0\",\n",
    "    \"Does the report use headings and subheadings to organize content? Yes=1, No=0\",\n",
    "    \"Does the report avoid information dumping and present ideas clearly? Yes=2, No=0\",\n",
    "    \"Does the report use precise and clear language to express ideas? Yes=2, No=0\",\n",
    "    \"Does the report avoid grammar, spelling, or sentence structure issues? Yes=1, No=0\",\n",
    "    \"Does the report demonstrate logical reasoning such as cause-effect or comparison? Yes=2, No=0\",\n",
    "    \"Does the report reflect critical thinking or independent judgment? Yes=2, No=0\",\n",
    "    \"Does the report conclude with insightful perspectives or calls to action? Yes=1, No=0\",\n",
    "    \"Does the report maintain a formal, academic, and objective tone throughout? Yes=1, No=0\",\n",
    "    \"Does the report cover all key aspects of the research topic? Yes=2, No=0\",\n",
    "    \"Does the report avoid missing important background or variables? Yes=1, No=0\",\n",
    "    \"Does the report provide sufficient evidence to support its claims? Yes=2, No=0\",\n",
    "    \"Does the report analyze underlying causes or trends in the data? Yes=2, No=0\",\n",
    "    \"Does the report incorporate multiple angles or dimensions in its analysis? Yes=1, No=0\",\n",
    "    \"Does the report demonstrate both breadth and depth of understanding? Yes=2, No=0\",\n",
    "    \"Does the report avoid vague or repetitive statements? Yes=1, No=0\",\n",
    "    \"Does the report cite authoritative academic journals or professional sources? Yes=2, No=0\",\n",
    "    \"Does the report provide clear citation formatting? Yes=1, No=0\",\n",
    "    \"Does the report cite sources that are highly relevant to the topic? Yes=2, No=0\",\n",
    "    \"Does the report avoid fabricated, unclear, or misleading references? Yes=2, No=0\",\n",
    "    \"Does the report embed citations within the body rather than only at the end? Yes=1, No=0\",\n",
    "    \"Does the report distinguish between primary and secondary sources? Yes=1, No=0\",\n",
    "    \"Does the report offer a unique perspective or analytical framework? Yes=2, No=0\",\n",
    "    \"Does the report critique existing viewpoints thoughtfully? Yes=2, No=0\",\n",
    "    \"Does the report propose innovative ideas or future research directions? Yes=2, No=0\",\n",
    "    \"Does the report show deep understanding of complex issues? Yes=2, No=0\",\n",
    "    \"Does the report avoid simply repeating existing conclusions? Yes=1, No=0\",\n",
    "    \"Does the report reflect the author’s reasoning and intellectual depth? Yes=2, No=0\",\n",
    "    \"Does the report use credible and verifiable data sources? Yes=2, No=0\",\n",
    "    \"Does the report interpret and explain data appropriately? Yes=2, No=0\",\n",
    "    \"Does the report use charts, tables, or visuals to support analysis? Yes=1, No=0\",\n",
    "    \"Does the report avoid misusing statistics or exaggerating findings? Yes=2, No=0\",\n",
    "    \"Does the report analyze data with causal or trend-based reasoning? Yes=2, No=0\",\n",
    "    \"Does the report acknowledge limitations or biases in the data? Yes=1, No=0\",\n",
    "    \"Does the report include source and date information for cited data? Yes=1, No=0\",\n",
    "    \"Does the report use proper Markdown heading levels (e.g., #, ##, ###)? Yes=1, No=0\",\n",
    "    \"Does the report use ordered or unordered lists to present key points? Yes=1, No=0\",\n",
    "    \"Does the report correctly use Markdown elements like code blocks, quotes, or tables? Yes=1, No=0\",\n",
    "    \"Does the report avoid Markdown syntax errors or formatting issues? Yes=1, No=0\",\n",
    "    \"Does the report maintain clean, readable, and visually consistent layout? Yes=1, No=0\",\n",
    "    \"Does the report use consistent terminology and avoid style shifts? Yes=1, No=0\",\n",
    "    \"Does the report avoid informal or conversational language? Yes=1, No=0\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Normalization Function\n",
    "def normalize_url(url):\n",
    "    parsed = urlparse(url.strip())\n",
    "    path = parsed.path.rstrip('/').lower()\n",
    "    normalized = urlunparse((parsed.scheme.lower(), parsed.netloc.lower(), path, '', '', ''))\n",
    "    return normalized\n",
    "\n",
    "# Get URL Hostname\n",
    "def extract_hostname(url):\n",
    "    parsed = urlparse(url.strip())\n",
    "    return parsed.netloc.lower()\n",
    "\n",
    "# Count the Number of URL Matches\n",
    "def count_matching_urls(refe, anno):\n",
    "    normalize_refe = list(set(normalize_url(url) for url in refe))\n",
    "    normalize_anno = list(set(normalize_url(url) for url in anno))\n",
    "    hostmane_normalize_refe = list(set(extract_hostname(refe_item) for refe_item in normalize_refe))\n",
    "    full_count = 0\n",
    "    for refe_item in normalize_refe:\n",
    "        for anno_item in normalize_anno:\n",
    "            if refe_item in anno_item:\n",
    "                full_count += 1\n",
    "    host_count = 0\n",
    "    for host_refe_item in hostmane_normalize_refe:\n",
    "        for anno_item in normalize_anno:\n",
    "            if host_refe_item in anno_item:\n",
    "                host_count += 1\n",
    "    return full_count, host_count\n",
    "\n",
    "# Count Keyword Occurrences\n",
    "def count_single_keyword(keyword, text):\n",
    "    text_lower = text.lower()\n",
    "    pattern = re.escape(keyword.lower())\n",
    "    count = len(re.findall(pattern, text_lower))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Log\n",
    "log_data = []\n",
    "with open(log_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        log = json.loads(line.strip())\n",
    "        log_data.append(log)\n",
    "\n",
    "for i in range(len(log_data)):\n",
    "    log = log_data[i]\n",
    "    uid = log[\"uid\"]\n",
    "    query = log[\"query\"]\n",
    "    qsr = log[\"qsr\"]\n",
    "    tsl = log[\"tsl\"]\n",
    "    fak = log[\"fak\"]\n",
    "    fdk = log[\"fdk\"]\n",
    "    grr = grr\n",
    "    input_tokens = log[\"input_tokens\"]\n",
    "    output_tokens = log[\"output_tokens\"]\n",
    "    reasoning_tokens = log[\"reasoning_tokens\"]\n",
    "    total_tokens = log[\"total_tokens\"]\n",
    "    annotations = log[\"annotations\"]\n",
    "    pure_annotations = log[\"pure_annotations\"]\n",
    "    text = log[\"text\"]\n",
    "    pure_text = log[\"pure_text\"]\n",
    "\n",
    "    # URL Match Evaluation\n",
    "    full_match, host_match = count_matching_urls(tsl, pure_annotations)\n",
    "    host_match = host_match - full_match\n",
    "    print(full_match, host_match)\n",
    "\n",
    "    # Keyword Match and Relevance Evaluation\n",
    "    keywords_judger_sprompt = \"\"\"\n",
    "        You are a scoring evaluator tasked with assessing the relevance of a specific keyword within a research report. You will be provided with:\n",
    "            1. A report text  \n",
    "            2. A keyword to evaluate\n",
    "        Your task is to:\n",
    "            - Read the report carefully\n",
    "            - Judge how semantically relevant the keyword is to the report content\n",
    "            - Consider not just frequency, but depth of discussion, thematic importance, and contextual integration\n",
    "        Use the following 5-point relevance scale:\n",
    "        [5] Extremely Relevant: The keyword is a central theme of the report; It appears multiple times and is discussed in depth; The report’s main arguments or findings revolve around it\n",
    "        [4] Highly Relevant: The keyword is a major topic; It appears more than once and is clearly explained or referenced; contributes directly to the report’s purpose\n",
    "        [3] Moderately Relevant: The keyword is mentioned but not emphasized; It may appear once or twice; It supports the report contextually but is not a focus\n",
    "        [2] Slightly Relevant: The keyword is briefly mentioned; It has little impact on the report’s core content; It may be incidental or peripheral\n",
    "        [1] Not Relevant: The keyword does not appear in the report; Or it appears in a way that is unrelated to the report’s topic\n",
    "        Output format example:\n",
    "            [4] The keyword “QUIC” is referenced multiple times in the report, particularly in the context of protocol evolution and RFC publication. While not the sole focus, it is clearly a major topic.\n",
    "        \"\"\"\n",
    "    keywords_judger = GPT(\"gpt-4o-2024-11-20\", sprompt=keywords_judger_sprompt)\n",
    "    \n",
    "    keywords_fak_count = []\n",
    "    keywords_fak_relevance = []\n",
    "    for keyword in fak:\n",
    "        fak_count = count_single_keyword(keyword, pure_text)\n",
    "        keywords_fak_count.append(fak_count)\n",
    "        rules_judger_uprompt = f\"\"\"\n",
    "            Report text: {pure_text} \n",
    "            Keyword: {keyword}\n",
    "            \"\"\"\n",
    "        fak_eval = keywords_judger(rules_judger_uprompt)\n",
    "        fak_score = int(re.findall(r\"\\[(\\d+)\\]\", fak_eval)[0])\n",
    "        keywords_fak_relevance.append(fak_score)\n",
    "    keywords_fdk_count = []\n",
    "    keywords_fdk_relevance = []\n",
    "    for keyword in fdk:\n",
    "        fdk_count = count_single_keyword(keyword, pure_text)\n",
    "        keywords_fdk_count.append(fdk_count)\n",
    "        rules_judger_uprompt = f\"\"\"\n",
    "            Report text: {pure_text} \n",
    "            Keyword: {keyword}\n",
    "            \"\"\"\n",
    "        fdk_eval = keywords_judger(rules_judger_uprompt)\n",
    "        fdk_score = int(re.findall(r\"\\[(\\d+)\\]\", fdk_eval)[0])\n",
    "        keywords_fdk_relevance.append(fdk_score)\n",
    "\n",
    "    # Rubrics Scoring and Evaluation\n",
    "    rules_judger_sprompt = \"\"\"\n",
    "        You are a scoring evaluator tasked with assessing the quality of a report generated by a deep research model. You will be provided with:\n",
    "            1. A report text  \n",
    "            2. An evaluation rule containing specific scoring criteria and allowed score values\n",
    "        Your task is to:\n",
    "            - Carefully read the report  \n",
    "            - Evaluate it strictly against the given rule  \n",
    "            - Assign a score based only on the score values defined in the rule\n",
    "        Scoring instructions:\n",
    "            - Only use the score values explicitly listed in the rule (e.g., 0, 1, 2)\n",
    "            - Do not invent intermediate scores or alternative formats\n",
    "            - Your output must begin with the score in square brackets [], followed by a one-sentence reason\n",
    "        Output format example:\n",
    "            [0] No citations were provided, which violates the requirement.\n",
    "            [2] The report fully meets the requirement with clear and relevant details.\n",
    "        Be objective and consistent. Focus on clarity, completeness, relevance, and adherence to the rule.\n",
    "        \"\"\"\n",
    "    rules_judger = GPT(\"gpt-4o-2024-11-20\", sprompt=rules_judger_sprompt)\n",
    "    \n",
    "    qsr_sum = 0\n",
    "    for rule in qsr:\n",
    "        rules_judger_uprompt = f\"\"\"\n",
    "            Report text: {text} \n",
    "            Rule: {rule}\n",
    "            \"\"\"\n",
    "        rules_eval = rules_judger(rules_judger_uprompt)\n",
    "        score = int(re.findall(r\"\\[(\\d+)\\]\", rules_eval)[0])\n",
    "        qsr_sum += score\n",
    "    grr_sum = 0\n",
    "    for rule in grr:\n",
    "        rules_judger_uprompt = f\"\"\"\n",
    "            Report text: {text} \n",
    "            Rule: {rule}\n",
    "            \"\"\"\n",
    "        rules_eval = rules_judger(rules_judger_uprompt)\n",
    "        score = int(re.findall(r\"\\[(\\d+)\\]\", rules_eval)[0])\n",
    "        grr_sum += score\n",
    "\n",
    "    # Eval Entry\n",
    "    eval = {\"uid\": uid, \"qsr_sum\": qsr_sum, \"grr_sum\": grr_sum,\n",
    "        \"keywords_fak_count\": keywords_fak_count, \"keywords_fak_relevance\": keywords_fak_relevance,\n",
    "        \"keywords_fdk_count\": keywords_fdk_count, \"keywords_fdk_relevance\": keywords_fdk_relevance,\n",
    "        \"url_full_match\": full_match, \"url_host_match\": host_match,\n",
    "        \"ref_url_num\": len(tsl), \"report_url_num\": len(list(set(pure_annotations)))}\n",
    "    # Save Eval Entry\n",
    "    with open(eval_file_path, 'a', encoding='utf-8') as outfile:\n",
    "        outfile.write(json.dumps(eval) + '\\n')\n",
    "    print(f\"Eval for UID {uid} has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a10d49",
   "metadata": {},
   "source": [
    "# Compute Core Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Eval\n",
    "eval_data = []\n",
    "with open(eval_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        eval = json.loads(line.strip())\n",
    "        eval_data.append(eval)\n",
    "\n",
    "quality_list = []\n",
    "semantic_list = []\n",
    "tboost_list = []\n",
    "intergrated_list = []\n",
    "\n",
    "for i in range(len(eval_data)):\n",
    "    eval = eval_data[i]\n",
    "    data_uid = eval[\"uid\"]\n",
    "    qsr_sum = eval[\"qsr_sum\"]\n",
    "    grr_sum = eval[\"grr_sum\"]\n",
    "    keywords_fak_count = eval[\"keywords_fak_count\"]\n",
    "    keywords_fak_relevance = eval[\"keywords_fak_relevance\"]\n",
    "    keywords_fdk_count = eval[\"keywords_fdk_count\"]\n",
    "    keywords_fdk_relevance = eval[\"keywords_fdk_relevance\"]\n",
    "    url_full_match = eval[\"url_full_match\"]\n",
    "    url_host_match = eval[\"url_host_match\"]\n",
    "    ref_url_num = eval[\"ref_url_num\"]\n",
    "    report_url_num = eval[\"report_url_num\"]\n",
    "\n",
    "    # Count QUA\n",
    "    quality = 0.5 * (qsr_sum / 30) + 0.5 * (grr_sum / 73)\n",
    "    quality_list.append(quality)\n",
    "\n",
    "    # Count SDR\n",
    "    fak_score = []\n",
    "    fdk_score = []\n",
    "    for i in range(5):\n",
    "        fak_score.append(min(keywords_fak_count[i] / 2, 1) * (keywords_fak_relevance[i] / 5))\n",
    "        fdk_score.append(min(keywords_fdk_count[i], 1) * (keywords_fdk_relevance[i] / 5))\n",
    "    fak_drift = 1 - (sum(fak_score) / 5)\n",
    "    fdk_drift = (sum(fdk_score) / 5)\n",
    "    semantic = 1 - (0.7 * fak_drift + 0.3 * fdk_drift)\n",
    "    semantic_list.append(semantic)\n",
    "\n",
    "    # Count TBO\n",
    "    tboost = 1 + 0.2 * (0.7 * (url_full_match / ref_url_num) + 0.3 * (url_host_match / (report_url_num + 1)))\n",
    "    tboost_list.append(tboost)\n",
    "\n",
    "    # Count ITS\n",
    "    intergrated = quality * semantic * tboost\n",
    "    intergrated_list.append(intergrated)\n",
    "\n",
    "# Print Metrics\n",
    "print(\n",
    "    f\"QUA:{np.mean(quality_list)}  SDR:{np.mean(semantic_list)}  TBO:{np.mean(tboost_list)}  ITS:{np.mean(intergrated_list)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f7767",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
