# <img src="Figures/rigbench_logo.png" alt="icon" width="45"/> A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports

### Preprint Not Released 

**Yang Yao Yixu Wang Yuxuan Zhang Yi Lu Tianle Gu Lingyu Li<br>Dingyi Zhao Keming Wu Haozhe Wang Ping Nie Yan Teng Yingchun Wang**<br>
*Shanghai Artificial Intelligence Laboratory The University of Hong Kong<br>Fudan University University of British Columbia University of Toronto<br>Tsinghua University  Shanghai Jiao Tong University<br>Hong Kong University of Science and Technology Peking University*


---

## 🧠 Abstract

As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.

## 🧪 Installation

```
git clone https://github.com/EVIGBYEN/RigorousBench.git
cd RigorousBench
```

