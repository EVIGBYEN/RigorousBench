# <img src="Figures/rigbench_logo.png" alt="icon" width="45"/> A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports

### Preprint Not Released 

**Yang Yao窶ズixu Wang窶ズuxuan Zhang窶ズi Lu窶サianle Gu窶キingyu Li<br>Dingyi Zhao窶ガeming Wu窶ォaozhe Wang窶ケing Nie窶ズan Teng窶ズingchun Wang**<br>
*Shanghai Artificial Intelligence Laboratory窶サhe University of Hong Kong<br>Fudan University窶ザniversity of British Columbia窶ザniversity of Toronto<br>Tsinghua University 窶ゴhanghai Jiao Tong University<br>Hong Kong University of Science and Technology窶ケeking University*


---

## 洫 Abstract

As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.

## 洫ｪ Installation

```
git clone https://github.com/EVIGBYEN/RigorousBench.git
cd RigorousBench
```

